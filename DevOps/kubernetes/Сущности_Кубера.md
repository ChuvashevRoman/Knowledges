# Архитектура Kubernetes

![Архитектура](./images/im1.png)

Основные элементы архитектуры Kubernetes:
- **Nodes** (node.md): Нода это (виртуальная) машина в кластере Kubernetes.
- **Pods** (pods.md): Pod это группа контейнеров с общими разделами, запускаемых как единое целое.
- **Controllers** - В Kubernetes контроллеры — это процессы управления, которые следят за состоянием вашего кластера, а затем при необходимости вносят или запрашивают изменения. Каждый контроллер пытается приблизить текущее состояние кластера к желаемому состоянию.

Также частью архитектуры являются:
- **ReplicaSets или Replication Controllers** (replication-controller.md): replication controller гарантирует, что определенное количество «реплик» pod'ы будут запущены в любой момент времени.
- **Deployments** - обеспечивает декларативные (declarative) обновления для Pods и ReplicaSets.
- **DaemonSet** - гарантирует, что определенный под будет запущен на всех (или некоторых) нодах.
- **Jobs (в том числе CronJob)** - создает один (или несколько) подов и гарантирует, что после выполнения команды они будут успешно завершены (terminated).
- **Services** (services.md): Сервис в Kubernetes это абстракция которая определяет логический объединённый набор pod и политику доступа к ним.
- **Volumes** (volumes.md): Volume(раздел) это директория, возможно, с данными в ней, которая доступна в контейнере.
- **Labels** (labels.md): Label'ы это пары ключ/значение которые прикрепляются к объектам, например pod'ам. Label'ы могут быть использованы для создания и выбора наборов объектов.
- **Kubectl Command Line Interface** (kubectl.md): kubectl интерфейс командной строки для управления Kubernetes.
- **Labels and Selectors** - пары ключ/значение, которые присваиваются объектам (например, подам). С помощью селекторов пользователь может идентифицировать объект;
- **Namespaces** - виртуальные кластеры размещенные поверх физического.
- **Annotations** - добавление произвольных неидентифицирующих метаданных к объектам.
- **ConfigMaps** - позволяет переопределить конфигурацию запускаемых подов.
- **Secrets** - используются для хранения конфиденциальной информации (пароли, токены, ssh-ключи).
- **Garbage Collection** - собирательный термин для различных механизмов, которые Kubernetes использует для очистки ресурсов кластера.

Работающий кластер Kubernetes включает в себя агента, запущенного на нодах (kubelet) и компоненты мастера (APIs, scheduler, etc), поверх решения с распределённым хранилищем. 

## Kubectl

Команды:

Выдать список всего ``kubectl get all -n <namespace>``
Список пространств ``kubectl get namespaces``
Список подов ``kubectl get pods -n <namespaces>``

## Node

## Pod

## ReplicaSet

ReplicaSet необходима для того, чтобы поддерживать стабильный набор реплик подов, запущенных в любой момент времени. Часто используется для обеспечения доступности определённого количества идентичных подов.

ReplicaSet определяется областью, включая селектор, указывающий, как идентифицировать поды, количество реплик, указывая сколько подов он должен поддерживать, шаблон пода, указывающий данные новых подов, которые он должен создать.
ReplicaSet удаляет или создаёт поды по мере необходимости для достижения желаемого количества. Создание пода происходит на основании шаблона.

Пример: 
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3
```



## Deployments

## Jobs

Job (работа, задание) — это yaml-манифест, который создаёт под для выполнения разовой задачи. Если запуск задачи завершается с ошибкой, Job перезапускает поды до успешного выполнения или до истечения таймаутов. Когда задача выполнена, Job считается завершённым и больше никогда в кластере не запускается. Job — это сущность для разовых задач.

**Jobs** необходима:
- При установке и настройке окружения. Например, мы построили CI/CD, который при создании новой ветки автоматически создаёт для неё окружение для тестирования. Появилась ветка — в неё пошли коммиты — CI/CD создал в кластере отдельный namespace и запустил Job — тот, в свою очередь, создал базу данных, налил туда данные, все конфиги сохранил в Secret и ConfigMap. 
- При выкатке helm chart. После развёртывания helm chart с помощью хуков (hook) запускается Job, чтобы проверить, как раскатилось приложение и работает ли оно.

Job будет создавать до тех пор, пока под не завершится с успешным результатом. Чтобы поды не создавались бесконечно в случае ошибки необходимо указывать ограничения на время **activeDeadlineSeconds** выполнения и количество выполнений **backoffLimit**. 

После успешного завершения задания манифесты Job и подов, созданных им, остаются в кластере навсегда. Все поля Job имеют статус Immutable, то есть «неизменяемый», и поэтому обычно при создании Job из различных автоматических сценариев сначала удаляют Job, который остался от предыдущего запуска. 
В Kubernetes есть специальный TTL Controller, который умеет удалять завершенные Job вместе с подами. ttlSecondsAfterFinished — указывает, через сколько секунд специальный TimeToLive контроллер должен удалить завершившийся Job вместе с подами и их логами.
Пример джобы выполняющей миграцию данных.
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: migrate-{{ $domain }}
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  activeDeadlineSeconds: {{ $.Values.JOB_activeDeadlineSeconds }}
  template:
    spec:
      imagePullSecrets:
        - name: prejob-registry-secret
      restartPolicy: Never
      containers:
        - name: {{ $.Release.Name}}-db-migrates
          image: "{{ $.Values.APP_IMAGE }}"
          command:
            - /bin/bash
            - -c
            - "cd /app && php artisan migrate --force"
          env:
            - name: "HTTP_HOST"
              value: "{{ $domain }}"
            - name: "DB_CONNECTION"
              value: "pgsql"
            - name: "DB_HOST"
              value: "{{ $.Values.DB_HOST }}"
            - name: DB_PORT
              value: "{{ $.Values.DB_PORT }}"
            - name: DB_SECRET
              value: "{{ $.Values.DB_SECRET }}"
            - name: RELEASE_VERSION
              value: "{{ $.Values.RELEASE_VERSION }}"
```

## Cronjob

CronJob — это yaml-манифест, на основании которого по расписанию создаются Job’ы, которые в свою очередь создают поды, а те делают полезную работу.


В манифесте CronJob указывают расписание и ещё несколько важных параметров.
- startingDeadlineSeconds,
- concurrencyPolicy.
- successfulJobsHistoryLimit,
- failedJobsHistoryLimit.
```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Allow
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 100
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: Never
```
**schedule** — это расписание в виде строчки, которая имеет обычный cron-формат. Строчка в примере говорит о том, что наш Job должен выполняться раз в минуту.
**concurrencyPolicy** — этот параметр отвечает за одновременное выполнение заданий. Бывает трёх видов: Allow, Forbid, Replace.
	**Allow** позволяет подам запускаться. Если за минуту Job не отработал, все равно будет создан ещё один. Одновременно могут выполняться несколько Job’ов.
	**Replace** заменяет запущенную нагрузку: старый Job убивается, запускается новый. На самом деле это не самый лучший вариант, когда прерываем то, что уже выполнялось, и начинаем ту же самую задачу выполнять заново. В каких-то случаях это возможно, в каких-то неприемлемо.
	**Forbid** запрещает запуск новых Job’ов, пока не отработает предыдущий. С этой политикой можно быть уверенным, что всегда запускается только один экземпляр задачи. Поэтому Forbid используют наиболее часто.

`jobTemplate` — это шаблон, из которого создаётся объект Job. Ну а всё остальное мы уже видели в манифесте Job.

В параметре **startingDeadlineSeconds** указывают количество секунд, на которое можно просрочить запуск Job. Если по каким-то причинам Job не создался и с момента, когда его надо было создать, прошло больше секунд, чем указано в этом параметре, то он и не будет создан. А если меньше, то хоть и с опозданием, Job будет создан. 

## DaemonSet

**Deamon Set** — объект API Kubernetes, который гарантирует, что определенный под будет запущен на всех (или некоторых) узлах. По мере добавления узлов в кластер к ним добавляются поды. Когда узлы удаляются из кластера, эти поды удаляются сборщиком мусора. Удаление DaemonSet приведет к очистке созданных им подов.

Типичные примеры использования DaemonSet:
- Запуск демона хранилища кластера на каждой ноде
- запуск демона сбора логов на каждой ноде
- запуск демона мониторинга узла

Обычно один DaemonSet покрывает все ноды, в которых должны быть использованы все типы демонов. Более сложная установка DaemonSets - для одного типа демонов, но с разными флагами и/или разными требованиями к памяти и ЦПУ для разного типа железа.

Пример конфигурации DaemonSets:
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
```

## ConfigMaps

ConfigMap — это объект API, используемый для хранения неконфиденциальных данных в парах ключ-значение. Поды могут использовать ConfigMaps как переменные среды, аргументы командной строки или как файлы конфигурации в volume.collection — это собирательный термин для различных механизмов, которые Kubernetes использует для очистки ресурсов кластера.

Основная информация в ConfigMaps хранится либо в поле data (кодировка UTF-8), либо binaryData (base64-encoded) 

Пример ConfigMaps:
```yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: game-demo
data:
  # property-like keys; each key maps to a simple value
  player_initial_lives: "3"
  ui_properties_file_name: "user-interface.properties"

  # file-like keys
  game.properties: |
    enemy.types=aliens,monsters
    player.maximum-lives=5    
  user-interface.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true   
```

Пример пода, использующего данный конфигмап:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-demo-pod
spec:
  containers:
    - name: demo
      image: alpine
      command: ["sleep", "3600"]
      env:
        # Define the environment variable
        - name: PLAYER_INITIAL_LIVES # Notice that the case is different here
                                     # from the key name in the ConfigMap.
          valueFrom:
            configMapKeyRef:
              name: game-demo           # The ConfigMap this value comes from.
              key: player_initial_lives # The key to fetch.
        - name: UI_PROPERTIES_FILE_NAME
          valueFrom:
            configMapKeyRef:
              name: game-demo
              key: ui_properties_file_name
      volumeMounts:
      - name: config
        mountPath: "/config"
        readOnly: true
  volumes:
    # You set volumes at the Pod level, then mount them into containers inside that Pod
    - name: config
      configMap:
        # Provide the name of the ConfigMap you want to mount.
        name: game-demo
        # An array of keys from the ConfigMap to create as files
        items:
        - key: "game.properties"
          path: "game.properties"
        - key: "user-interface.properties"
          path: "user-interface.properties"
```

ConfigMap может быть смонтированна как volumes. 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    configMap:
      name: myconfigmap
```





